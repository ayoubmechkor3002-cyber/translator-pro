"""
Gestion du cache des mod√®les MarianMT pour optimiser les performances
"""
from transformers import MarianMTModel, MarianTokenizer
import torch
from typing import Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelCache:
    """Cache intelligent pour les mod√®les de traduction"""
    
    def __init__(self):
        self.models: Dict[str, MarianMTModel] = {}
        self.tokenizers: Dict[str, MarianTokenizer] = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üíª Device utilis√©: {self.device}")
        
        # Mapping des paires de langues vers les mod√®les MarianMT
        self.model_mapping = {
            "fr-en": "Helsinki-NLP/opus-mt-fr-en",
            "en-fr": "Helsinki-NLP/opus-mt-en-fr",
            "ar-en": "Helsinki-NLP/opus-mt-ar-en",
            "en-ar": "Helsinki-NLP/opus-mt-en-ar",
            "fr-ar": "Helsinki-NLP/opus-mt-fr-ar",
            "ar-fr": "Helsinki-NLP/opus-mt-ar-fr",
            "es-en": "Helsinki-NLP/opus-mt-es-en",
            "en-es": "Helsinki-NLP/opus-mt-en-es",
            "de-en": "Helsinki-NLP/opus-mt-de-en",
            "en-de": "Helsinki-NLP/opus-mt-en-de",
            "it-en": "Helsinki-NLP/opus-mt-it-en",
            "en-it": "Helsinki-NLP/opus-mt-en-it",
        }
    
    def get_model_name(self, source_lang: str, target_lang: str) -> str:
        """Obtient le nom du mod√®le pour une paire de langues"""
        pair = f"{source_lang}-{target_lang}"
        if pair in self.model_mapping:
            return self.model_mapping[pair]
        raise ValueError(f"‚ùå Paire de langues non support√©e: {pair}")
    
    def load_model(self, source_lang: str, target_lang: str) -> Tuple[MarianMTModel, MarianTokenizer]:
        """Charge un mod√®le depuis le cache ou depuis Hugging Face"""
        pair = f"{source_lang}-{target_lang}"
        
        # Si le mod√®le est d√©j√† en cache
        if pair in self.models:
            logger.info(f"‚úÖ Mod√®le {pair} charg√© depuis le cache")
            return self.models[pair], self.tokenizers[pair]
        
        # Sinon, charger le mod√®le
        try:
            model_name = self.get_model_name(source_lang, target_lang)
            logger.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le: {model_name}")
            
            tokenizer = MarianTokenizer.from_pretrained(model_name)
            model = MarianMTModel.from_pretrained(model_name).to(self.device)
            
            # Mettre en cache
            self.models[pair] = model
            self.tokenizers[pair] = tokenizer
            
            logger.info(f"‚úÖ Mod√®le {pair} charg√© avec succ√®s")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
            raise
    
    def clear_cache(self):
        """Vide le cache des mod√®les"""
        self.models.clear()
        self.tokenizers.clear()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("üóëÔ∏è Cache vid√©")


# Instance globale du cache
model_cache = ModelCache()
